{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3678124",
   "metadata": {},
   "source": [
    "# Simple binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53086d25",
   "metadata": {},
   "source": [
    "In this notebook we study the classification problem using different techniques and a dataset based on sonar-based object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ae7a3",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the\n",
    "strength of sonar returns at different angles. It is a binary (2-class) classification problem. The\n",
    "number of observations for each class is not balanced. There are 208 observations with 60 input\n",
    "variables and 1 output variable.\n",
    "\n",
    "The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar\n",
    "signals off a metal cylinder at various angles and under various\n",
    "conditions.  The file \"sonar.rocks\" contains 97 patterns obtained from\n",
    "rocks under similar conditions.  The transmitted sonar signal is a\n",
    "frequency-modulated chirp, rising in frequency.  The data set contains\n",
    "signals obtained from a variety of different aspect angles, spanning 90\n",
    "degrees for the cylinder and 180 degrees for the rock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe33a7",
   "metadata": {},
   "source": [
    "## Code requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e82ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using StatsBase\n",
    "using Printf\n",
    "using ScikitLearn\n",
    "using ScikitLearn: fit!, predict\n",
    "using StatsBase: sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0569f",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294cc87",
   "metadata": {},
   "source": [
    "The data set is processed and divided into two parts:\n",
    "- A first section consisting of 70 random samples of objects classified as mines and 70 more of objects classified as rocks.\n",
    "- A testing section containing the remaining elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370b320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0335, 0.0307, 0.0116, 0.0331, 0.0428, 0.0094, 0.0228, 0.0363, 0.0261, 0.0162, 0.0249, 0.027, 0.0209, 0.0374, 0.0443, 0.0968, 0.079, 0.0731, 0.0164, 0.0707, 0.0526, 0.0721, 0.0654, 0.0207, 0.0209, 0.0131, 0.0117, 0.0258, 0.0217, 0.0163, 0.0221, 0.0411, 0.013, 0.018, 0.0635, 0.0201, 0.034, 0.0209, 0.0368, 0.0315, 0.0056, 0.0392, 0.0129, 0.0366, 0.0116, 0.0131, 0.0272, 0.0187, 0.0323, 0.0522, 0.0303, 0.01, 0.0317, 0.0164, 0.0039, 0.0079, 0.009, 0.0126, 0.0293, 0.0177, 0.0189, 0.0084, 0.0311, 0.0333, 0.0068, 0.0093, 0.0373, 0.0119, 0.0131, 0.0293, 0.0152, 0.0216, 0.0225, 0.013, 0.0067, 0.0216, 0.0208, 0.0139, 0.0202, 0.0239, 0.0336, 0.0409, 0.0188, 0.0856, 0.0126, 0.0253, 0.0025, 0.0291]"
     ]
    }
   ],
   "source": [
    "myPath = \"Data/sonar.csv\"\n",
    "function trainingSelection(n, path=myPath)\n",
    "    \n",
    "    sonarRaw = CSV.read(path, DataFrame)\n",
    "    sonar = sort!(sonarRaw, [:61])\n",
    "    \n",
    "    k = size(sonar,1)\n",
    "    m = findfirst(isequal(\"R\"), sonar[!,:61])\n",
    "    \n",
    "    if (n > (k-m) || n > (m-1))\n",
    "        print(\"There is insufficient data for a balanced sample of n \")\n",
    "        return nothing\n",
    "    end\n",
    "    \n",
    "    RowsMine = sample(1:(m-1), n, replace=false, ordered=true) # Muestra de indices aleatorios donde la linea tiene clasificación M\n",
    "    sonarMine = sonar[RowsMine, :] # Dadaframe con las filas resultantes de RowsMine\n",
    "    \n",
    "    RowsRock = sample(m:k, n, replace=false, ordered=true) # Muestra de indices aleatorios donde la linea tiene clasificación R\n",
    "    sonarRock = sonar[RowsRock, :] #  Dadaframe con las filas resultantes de RowsMine\n",
    "    \n",
    "    testData = sonar[Not(union(RowsMine, RowsRock)),:] # Dataframe con las filas que no estan ni en RowsMine ni RowsRock\n",
    "    \n",
    "    trainingData = vcat(sonarMine, sonarRock); # Union de sonarMine con sonarRock (Conjunto de entrenamiento)\n",
    "    \n",
    "    return trainingData, testData\n",
    "    \n",
    "end\n",
    "\n",
    "out = trainingSelection(60) # Muestra de datos \n",
    "training = out[1] # Entrenamiento\n",
    "testing = out[2] # Prueba\n",
    "\n",
    "training.\"f61\" .= replace.(training.\"f61\", \"M\" => \"1\") # Conversion de la clasificación\n",
    "training.\"f61\" .= replace.(training.\"f61\", \"R\" => \"0\")\n",
    "training.\"f61\" = parse.(Float64, training.\"f61\")\n",
    "training[training.\"f61\" .< 0.5,:f61] .= -1.0;\n",
    "\n",
    "testing.\"f61\" .= replace.(testing.\"f61\", \"M\" => \"1\")\n",
    "testing.\"f61\" .= replace.(testing.\"f61\", \"R\" => \"0\")\n",
    "testing.\"f61\" = parse.(Float64, testing.\"f61\")\n",
    "testing[testing.\"f61\" .< 0.5,:f61] .= -1.0;\n",
    "\n",
    "print(testing.\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d152bfb",
   "metadata": {},
   "source": [
    "Additionally the representation of the two classes by positive cases (mines -> 1) and negative cases (rocks -> -1) is transformed and the data is dumped into two new files to persist the data in case of further testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b3d8b",
   "metadata": {},
   "source": [
    "## Dump Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f0c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"Data/training.csv\", training)\n",
    "CSV.write(\"Data/test.csv\", testing);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af94bd",
   "metadata": {},
   "source": [
    "# Step 2: Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0841d9",
   "metadata": {},
   "source": [
    "## Confussion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad7274",
   "metadata": {},
   "source": [
    "Confusion Matrix is a tabular visualization of the ground-truth labels versus model predictions. Each row of the confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class. Confusion Matrix is not exactly a performance metric but sort of a basis on which other metrics evaluate the results.\n",
    "Each cell in the confusion matrix represents an evaluation factor:\n",
    "\n",
    "- True Positive(TP) signifies how many positive class samples your model predicted correctly.\n",
    "- True Negative(TN) signifies how many negative class samples your model predicted correctly.\n",
    "- False Positive(FP) signifies how many negative class samples your model predicted incorrectly. This factor represents Type-I error in statistical nomenclature. This error positioning in the confusion matrix depends on the choice of the null hypothesis.\n",
    "- False Negative(FN) signifies how many positive class samples your model predicted incorrectly. This factor represents Type-II error in statistical nomenclature. This error positioning in the confusion matrix also depends on the choice of the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ff91c",
   "metadata": {},
   "source": [
    "## Selected Metric\n",
    "\n",
    "It is determined that **accuracy** will be the metric used to evaluate the performance of the machines to be considered:\n",
    "\n",
    "Accuracy is one of the metrics for evaluating classification models in ML. It can be explained as the fraction of predictions that our model got right. Formally, we define it as follows:\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec719cb",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "With the data processed and separated into training and testing, we proceed to study the following algorithms applied to this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78918252",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Array(training)[:, 1:60] # Separación de variables de caracteristica y etiquetas\n",
    "y_train = Array(training)[:,61]\n",
    "\n",
    "x_test = Array(testing)[:, 1:60]\n",
    "y_test = Array(testing)[:, 61];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba7ccc",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6504d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.136%\n"
     ]
    }
   ],
   "source": [
    "@sk_import linear_model: LinearRegression\n",
    "\n",
    "linReg_model = LinearRegression()\n",
    "\n",
    "fit!(linReg_model, x_train, y_train)\n",
    "\n",
    "prediction = predict(linReg_model, x_test)\n",
    "prediction_class = [if x < 0 -1 else 1 end for x in prediction]\n",
    "@printf \"Accuracy: %.3f%%\\n\" mean((prediction_class .== y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bd1f9",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53573ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.955%\n"
     ]
    }
   ],
   "source": [
    "@sk_import linear_model: LogisticRegression\n",
    "\n",
    "logReg_model = LogisticRegression()\n",
    "\n",
    "fit!(logReg_model, x_train, y_train)\n",
    "\n",
    "prediction = predict(logReg_model, x_test)\n",
    "prediction_class = [if x < 0 -1 else 1 end for x in prediction]\n",
    "\n",
    "@printf \"Accuracy: %.3f%%\\n\" mean((prediction_class .== y_test))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ccd839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = size(y_train,1) # Separación de datos de entrenamiento con el fin de hallar hiperparametros en svm y knn\n",
    "function t1_t2_selector(m,t)\n",
    "    sam = sample(1:m, t, replace=false, ordered=true)\n",
    "    return sam\n",
    "end\n",
    "\n",
    "r = t1_t2_selector(m1,30) # Muestra de 30 datos para hallar hiperparametros\n",
    "\n",
    "t1 = training[r,:]\n",
    "t2 = training[Not(r),:]\n",
    "    \n",
    "x_htrain = Array(t1)[:, 1:60]\n",
    "y_htrain = Array(t1)[:,61]\n",
    "\n",
    "x_train = Array(training)[:, 1:60]\n",
    "y_train = Array(training)[:,61];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ed9b6",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bffdcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de C es: 7.800000000000001\n",
      "Accuracy: 90.909%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "@sk_import svm: SVC\n",
    "\n",
    "aciertos_c = []\n",
    "for i in 0.1:0.1:100 # Busqueda del hiperparametro entre 0.1 y 100\n",
    "    svm = SVC( C = i)\n",
    "    fit!(svm, x_htrain, y_htrain)\n",
    "\n",
    "    svm_prediccion = predict(svm, x_test)\n",
    "\n",
    "    svm_point = mean((svm_prediccion .== y_test))*100\n",
    "\n",
    "    push!(aciertos_c, svm_point)\n",
    "end\n",
    "\n",
    "Cop = 0.1*(1 + argmax(aciertos_c)) # Valor óptimo del hiperparametro en la metrica escogida\n",
    "println(\"El valor de C es: $Cop\")\n",
    "\n",
    "\n",
    "svm = SVC( C = Cop) \n",
    "fit!(svm, x_train, y_train) \n",
    "svm_prediction = predict(svm, x_test)\n",
    "\n",
    "@printf \"Accuracy: %.3f%%\\n\" mean((svm_prediction .== y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10071b02",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c19d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.000%\n"
     ]
    }
   ],
   "source": [
    "@sk_import neighbors: KNeighborsClassifier\n",
    "\n",
    "aciertos_c = []\n",
    "for i in 1:10 # Busqueda del hiperparametro entre 1 y 10\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    fit!(knn,x_htrain, y_htrain)\n",
    "    knn_prediccion = predict(knn, x_test)\n",
    "\n",
    "    knn_point = mean((knn_prediccion .== y_test))*100\n",
    "    push!(aciertos_c, knn_point)\n",
    "end\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = argmax(aciertos_c))\n",
    "fit!(knn, x_train, y_train)\n",
    "knn_prediction = predict(knn, x_test)\n",
    "\n",
    "@printf \"Accuracy: %.3f%%\\n\" mean((knn_prediction .== y_test))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123a6f8",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635c6b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.136%\n"
     ]
    }
   ],
   "source": [
    "@sk_import tree: DecisionTreeClassifier\n",
    "\n",
    "arb_modelo = DecisionTreeClassifier()\n",
    "\n",
    "fit!(arb_modelo, x_train, y_train) \n",
    "\n",
    "arb_prediccion = predict(arb_modelo, x_test);\n",
    "\n",
    "@printf \"Accuracy: %.3f%%\\n\" mean((arb_prediccion .== testing.\"f61\"))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b85e3",
   "metadata": {},
   "source": [
    "## Best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba35dc",
   "metadata": {},
   "source": [
    "Each implementation of the algorithms considers the calculation of performance metrics. We see then that in this case the SVM algorithm is the one that best classified the training data (with an accuracy of 90.9%)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
